{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5685950,"sourceType":"datasetVersion","datasetId":3268706}],"dockerImageVersionId":30920,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-26T09:47:39.047797Z","iopub.status.idle":"2025-02-26T09:47:39.048152Z","shell.execute_reply":"2025-02-26T09:47:39.047992Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"p=[]\nwith open(\"/kaggle/input/sherleck-books/a_study_in_scarlet.txt\",\"r\") as f:\n    p=f.read()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T09:48:17.488057Z","iopub.execute_input":"2025-02-26T09:48:17.488375Z","iopub.status.idle":"2025-02-26T09:48:17.505214Z","shell.execute_reply.started":"2025-02-26T09:48:17.488353Z","shell.execute_reply":"2025-02-26T09:48:17.504372Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\n\ntext = p\n\ntokens = nltk.word_tokenize(text.lower())\n\nvocab = {word: idx for idx, word in enumerate(set(tokens))}\nvocab_size = len(vocab)\n\n#print(\"Vocabulary:\", vocab)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T09:48:18.072944Z","iopub.execute_input":"2025-02-26T09:48:18.073252Z","iopub.status.idle":"2025-02-26T09:48:18.389074Z","shell.execute_reply.started":"2025-02-26T09:48:18.073229Z","shell.execute_reply":"2025-02-26T09:48:18.388452Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Generate (target, context) pairs for Skip-gram using a simple sliding window\nwindow_size = 2\nskip_gram_pairs = []\n\nfor i, target in enumerate(tokens):\n    for j in range(max(0, i - window_size), min(len(tokens), i + window_size + 1)):\n        if i != j:\n            skip_gram_pairs.append((vocab[target], vocab[tokens[j]]))\n\n#print(\"Skip-gram pairs:\", skip_gram_pairs)\n\n# For CBOW, prepare (context, target) pairs:\ncbow_pairs = []\nfor i, target in enumerate(tokens):\n    context = []\n    for j in range(max(0, i - window_size), min(len(tokens), i + window_size + 1)):\n        if i != j:\n            context.append(vocab[tokens[j]])\n    if context:\n        cbow_pairs.append((context, vocab[target]))\n\n#print(\"CBOW pairs:\", cbow_pairs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T09:48:21.615856Z","iopub.execute_input":"2025-02-26T09:48:21.616154Z","iopub.status.idle":"2025-02-26T09:48:21.954668Z","shell.execute_reply.started":"2025-02-26T09:48:21.616133Z","shell.execute_reply":"2025-02-26T09:48:21.953964Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model\n\nembedding_dim = 50\n\n# Define a simple Skip-gram model\nclass SkipGramModel(Model):\n    def __init__(self, vocab_size, embedding_dim):\n        super(SkipGramModel, self).__init__()\n        self.embedding = layers.Embedding(vocab_size, embedding_dim, input_length=1)\n        self.dense = layers.Dense(vocab_size, activation='softmax')\n    \n    def call(self, inputs):\n        x = self.embedding(inputs)\n        x = tf.reshape(x, (-1, embedding_dim))\n        return self.dense(x)\n\n# Create model\nskipgram_model = SkipGramModel(vocab_size, embedding_dim)\nskipgram_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n\n# Example training data for Skip-gram\nimport numpy as np\n# Separate targets and contexts from pairs\ntargets = np.array([pair[0] for pair in skip_gram_pairs])\ncontexts = np.array([pair[1] for pair in skip_gram_pairs])\n\n# Train the model (for demonstration, use a small number of epochs)\nskipgram_model.fit(targets, contexts, epochs=10, batch_size=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T09:48:24.466037Z","iopub.execute_input":"2025-02-26T09:48:24.466327Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m111895/111895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 1ms/step - loss: 6.8623\nEpoch 2/10\n\u001b[1m  3790/111895\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:55\u001b[0m 1ms/step - loss: 6.5489","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Define a simple CBOW model: average context embeddings to predict the target word.\nclass CBOWModel(Model):\n    def __init__(self, vocab_size, embedding_dim):\n        super(CBOWModel, self).__init__()\n        self.embedding = layers.Embedding(vocab_size, embedding_dim, input_length=None)\n        self.dense = layers.Dense(vocab_size, activation='softmax')\n    \n    def call(self, inputs):\n        # inputs shape: (batch_size, context_window)\n        x = self.embedding(inputs)\n        # Average over context words\n        x = tf.reduce_mean(x, axis=1)\n        return self.dense(x)\n\ncbow_model = CBOWModel(vocab_size, embedding_dim)\ncbow_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n\n# Prepare CBOW training data: for simplicity, pad context list to fixed size (here: window_size*2)\nimport numpy as np\n\nmax_context = window_size * 2\ndef pad_context(context_list, max_len):\n    return context_list + [0]*(max_len - len(context_list))\n\ncontexts_cbow = np.array([pad_context(c, max_context) for c, _ in cbow_pairs])\ntargets_cbow = np.array([target for _, target in cbow_pairs])\n\ncbow_model.fit(contexts_cbow, targets_cbow, epochs=10, batch_size=2)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math\n\n# Calculate perplexity for the Skip-gram model\nskip_loss = skipgram_model.evaluate(targets, contexts, verbose=0)\nskip_perplexity = math.exp(skip_loss)\nprint(\"Skip-gram Perplexity:\", skip_perplexity)\n\n# Calculate perplexity for the CBOW model\ncbow_loss = cbow_model.evaluate(contexts_cbow, targets_cbow, verbose=0)\ncbow_perplexity = math.exp(cbow_loss)\nprint(\"CBOW Perplexity:\", cbow_perplexity)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Choose a sample target word and get its index\nsample_word = \"sofa\"\nsample_index = vocab[sample_word] if sample_word in vocab else list(vocab.values())[0]\n\n# Predict context word probabilities from the sample target\npredicted_probs = skipgram_model.predict(np.array([sample_index]))\npredicted_context_index = np.argmax(predicted_probs, axis=-1)[0]\n\n# Find the corresponding word from the vocabulary\npredicted_context_word = [word for word, idx in vocab.items() if idx == predicted_context_index][0]\nprint(\"Skip-gram prediction - For target word '{}', predicted context word: '{}'\".format(sample_word, predicted_context_word))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use the first CBOW pair as a sample\nsample_context_indices, true_target_index = cbow_pairs[0]\n\n# Pad the context to fixed length (max_context defined earlier)\nsample_context_padded = pad_context(sample_context_indices, max_context)\nsample_context_padded = np.array([sample_context_padded])\n\n# Predict target word probabilities from the context\npredicted_target_probs = cbow_model.predict(sample_context_padded)\npredicted_target_index = np.argmax(predicted_target_probs, axis=-1)[0]\n\n# Find the corresponding word from the vocabulary\npredicted_target_word = [word for word, idx in vocab.items() if idx == predicted_target_index][0]\nprint(\"CBOW prediction - For context words {} , predicted target word: '{}'\".format(sample_context_indices, predicted_target_word))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}